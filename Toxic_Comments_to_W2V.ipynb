{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>\n",
    "Toxic Comments Kaggle Competition Data Preperation\n",
    "</center></h1>\n",
    "<h2><center>\n",
    "Preparing the Data to Use in Modeling\n",
    "</center></h2>\n",
    "<h3><center>\n",
    "Marcel Colvin 912033961\n",
    "</center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import nltk\n",
    "from nltk import corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer as wnl\n",
    "import re\n",
    "import gensim\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Toxic_comments/train.csv')\n",
    "test = pd.read_csv('Toxic_comments/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments: 159571\n",
      "toxic: 15294\n",
      "severe_toxic: 1595\n",
      "obscene: 8449\n",
      "threat: 478\n",
      "insult: 7877\n",
      "identity_hate: 1405\n"
     ]
    }
   ],
   "source": [
    "print('Total comments: {}'.format(len(train)))\n",
    "print('toxic: {}'.format(train[train['toxic'] > 0]['toxic'].count()))\n",
    "print('severe_toxic: {}'.format(train[train['severe_toxic'] > 0]['severe_toxic'].count()))\n",
    "print('obscene: {}'.format(train[train['obscene'] > 0]['obscene'].count()))\n",
    "print('threat: {}'.format(train[train['threat'] > 0]['threat'].count()))\n",
    "print('insult: {}'.format(train[train['insult'] > 0]['insult'].count()))\n",
    "print('identity_hate: {}'.format(train[train['identity_hate'] > 0]['identity_hate'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['length'] = train['comment_text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    lemmatized = wnl().lemmatize(word.lower())\n",
    "    return word\n",
    "def tokenize(text):\n",
    "    new_text = re.sub(\"[^0-9a-zA-Z]\", ' ', text)\n",
    "    new_list = new_text.lower().split()\n",
    "    stop = stopwords.words('english')\n",
    "    words = [x for x in new_list if not x in stop]\n",
    "    final = [lemmatize(x) for x in words]\n",
    "    return final\n",
    "train['tokens'] = train[\"comment_text\"].apply(tokenize)\n",
    "test['tokens'] = test['comment_text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_model = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_vector(tokens, vector, gen_missing=False, k=300):\n",
    "    if len(tokens)<1:\n",
    "        return np.zeros(k)\n",
    "    if gen_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens]\n",
    "    average_word = np.divide(np.sum(vectorized, axis=0), len(vectorized))\n",
    "    return average_word\n",
    "\n",
    "train[\"avg_word\"] = list(train['tokens'].apply(lambda x: get_average_vector(x, google_model, gen_missing = True)))\n",
    "test[\"avg_word\"] = list(test['tokens'].apply(lambda x: get_average_vector(x, google_model, gen_missing = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"training_data.csv\", index = False)\n",
    "test.to_csv(\"test_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
